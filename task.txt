Project Goal
Build an AI agent capable of answering specific questions based on a collection of academic research papers (PDFs) provided by the user. The system will leverage RAG (Retrieval-Augmented Generation) to find relevant content within documents and generate coherent, sourced answers via an Ollama-served LLM.
Technical Framework
•	Backend: Python Flask application
•	Containerization: Docker and Docker Compose
•	Vector Database: ChromaDB for document embeddings
•	Storage: MongoDB for query logging
•	LLM: Custom Ollama model
•	API Documentation: Swagger/OpenAPI
•	Validation: Pydantic for request/response validation
Key Components
1. Docker Infrastructure
•	All services containerized and orchestrated via Docker Compose: 
o	Flask application (custom image)
o	Ollama with custom model (based on Modelfile)
o	ChromaDB for vector storage
o	MongoDB for logging
o	Mongo Express for database GUI
2. Flask API Endpoints
PDF Upload Endpoint
•	Route: POST /papers
•	Function: Accepts multiple PDF research papers
•	Processing: 
o	Extract text from PDFs
o	Split into manageable chunks
o	Generate embeddings via OllamaEmbeddings
o	Store in ChromaDB with metadata (document name, chunk ID)
•	Response: Success status and document IDs
Query Endpoint
•	Route: POST /query
•	Function: Accepts natural language questions about the uploaded papers
•	Processing: 
o	Validate input with Pydantic
o	Convert question to embedding
o	Retrieve relevant document chunks from ChromaDB
o	Submit to LLM with appropriate context and instructions
o	Parse and validate output
o	Log interaction in MongoDB
•	Response: Structured answer with source citations
3. Custom Ollama Model
•	Create a custom Modelfile to configure: 
o	Base model (e.g., llama2, mistral)
o	System prompt optimized for academic Q&A
o	Parameters for balanced response quality and speed
4. Advanced RAG Implementation
•	Sophisticated prompt engineering for context-aware responses
•	Citation tracking in retrieved documents
•	Metadata management to trace source documents
•	Proper handling of multiple document contexts
5. Data Validation & Structured Responses
•	Pydantic models for: 
o	Query validation (ensuring valid questions)
o	Response structuring (formatted answers with citations)
•	Custom output parser to extract: 
o	Main answer text
o	Source citations (document names)
o	Confidence scores (optional)
6. Comprehensive Logging
•	MongoDB collection for detailed query logs: 
o	Timestamp
o	User query
o	Retrieved document chunks
o	Generated answer
o	Source citations
o	Processing metadata
o	Performance metrics
7. API Documentation
•	Interactive Swagger/OpenAPI interface
•	Endpoint documentation with example requests/responses
•	Model schema documentation
File Structure
project_root/
├── docker-compose.yml           # Orchestrates all containers
├── Dockerfile                   # Flask app container definition
├── modelfile/                   # Custom Ollama model definition
│   └── Modelfile
├── app/                         # Flask application
│   ├── __init__.py
│   ├── main.py                  # Core application and routes
│   ├── config.py                # Configuration settings
│   ├── models.py                # Pydantic data models
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── pdf_processor.py     # PDF extraction and chunking
│   │   ├── embeddings.py        # Embedding generation utilities
│   │   └── logging.py           # MongoDB logging functions
│   ├── database/
│   │   ├── __init__.py
│   │   ├── chroma_client.py     # ChromaDB interaction
│   │   └── mongo_client.py      # MongoDB interaction
│   ├── rag/
│   │   ├── __init__.py
│   │   ├── prompt_templates.py  # LLM prompt engineering
│   │   ├── chain.py             # RAG implementation
│   │   └── output_parser.py     # Response structuring
│   └── api/
│       ├── __init__.py
│       ├── endpoints.py         # API route definitions
│       └── swagger.py           # API documentation setup
├── requirements.txt             # Python dependencies
└── README.md                    # Project documentation
